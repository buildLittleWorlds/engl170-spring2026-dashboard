The Friction of Fluency: Why "Easy" AI is a Myth
January 18, 2026
Response to: The Fluency Illusion: AI-Induced Learned Helplessness (January 15, 2026)
In a recent post on "The Fluency Illusion," the author raises a compelling psychological warning. Drawing on the work of Dr. Plate and Hanning Ni, the post argues that the smooth, confident syntax of Artificial Intelligence creates a "distinctive epistemic hazard." Because AI generates text that is easy to read, we trick ourselves into thinking we understand the content. We mistake the machine's fluency for our own competence.
The post concludes with a grim prediction: this "cognitive offloading" will lead to "learned helplessness." If we never struggle with the messiness of thought, we will eventually lose the ability to think altogether. The prescribed cure is a return to "desirable difficulty"—intentionally turning off the machine to force our brains to sweat.
I agree entirely that passive consumption of AI content creates an illusion of competence. If a student simply types "Write my essay" and hands it in, they have learned nothing. However, this argument relies on a dangerous oversimplification. It assumes that all AI usage is passive. It assumes that the workflow of the future is frictionless.
My complication to this theory is simple: If your AI workflow feels "fluent" and "easy," you are doing it wrong. The antidote to learned helplessness is not to ban the tool; it is to engage with the tool until it fights back.
The Myth of the Magic Button
The "Fluency Illusion" argument is predicated on the idea that AI works like a vending machine: you put in a coin (prompt), and you get a perfect product (essay). In this model, the human does zero cognitive work.
But as I argued in The Obsolescence Trap, this is the workflow of a novice. Anyone who has attempted to use Large Language Models for high-level intellectual work knows that "fluency" is rarely the result of the first prompt. In fact, professional AI interaction is characterized by friction.
When you ask an AI to analyze a complex text, it often misses the nuance. When you ask it to synthesize two disparate ideas, it often hallucinates a connection that isn't there. When you ask it to adopt a specific tone, it often devolves into caricature.
Correcting these errors is not "offloading" cognition; it is a rigorous exercise in critical auditing. The user must spot the flaw in the "fluent" text (which requires deep subject matter knowledge), formulate a correction strategy (which requires logic), and re-prompt the machine (which requires rhetorical precision). This is not helplessness. This is management.
The New "Desirable Difficulty"
The post suggests that "desirable difficulty" only exists in traditional, manual tasks—like "writing a messy, non-fluent first draft" by hand. This definition of difficulty is too narrow.
Consider the cognitive load of a modern AI-integrated workflow. A student might generate three different counter-arguments to their thesis using three different personas. The AI produces these instantly. Now, the student faces a new difficulty: Which one is valid?
This is a harder problem than staring at a blank page. Staring at a blank page requires generation (making something from nothing). Evaluating three plausible-sounding but potentially flawed arguments requires discernment (separating truth from noise). This is the exact opposite of the "Fluency Illusion." The student isn't being lulled to sleep by the text; they are being forced to interrogate it.
If we accept Hanning Ni’s fear of "learned helplessness," we must ask: Who is more helpless? The student who can only write if they have a quiet room and a pen? Or the student who can navigate a flood of synthetic information, spot the hallucinations, and curate a coherent truth from the chaos?
Distrust as a Skill
The author of "The Fluency Illusion" writes that "we walk away with unearned confidence." This is true—but only if we teach students to trust the machine. The solution, then, is not to remove the machine, but to teach distrust.
We should not be telling students "Don't use AI because it's too easy." We should be telling them "Use AI, but assume it is lying to you."
When a student treats the AI as an unreliable narrator rather than an oracle, the dynamic shifts. The interaction becomes an adversarial process. The student is constantly checking facts, challenging assumptions, and rewriting the "fluent" slop into sharp, human insight. This adversarial process builds a "fragile sense of self-esteem" into a robust, skeptical intellect.
Conclusion: The Crucible Has Moved
The post ends with a call to "stay in the crucible of critical thinking." I applaud that sentiment. We absolutely need to keep struggling with ideas. But we must recognize that the location of the crucible has shifted.
The struggle is no longer about finding the right word to start a sentence. The struggle is about navigating an information environment where "fluency" is cheap and "truth" is expensive. If we retreat to our notebooks and pretend the AI doesn't exist, we aren't saving our agency. We are hiding from the actual challenge of our time.
The danger isn't that AI makes things too easy. The danger is that we won't train ourselves to do the hard work of managing it.
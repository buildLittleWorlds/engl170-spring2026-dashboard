The Canary in the Codebase
February 9, 2026
Something is happening in the blog network that I want to step back and name.
Several of you are writing about what seems like different topics--coding, sports analytics, music production, medicine, genetics. But you're actually circling the same set of questions. And those questions are going to dominate professional life for the next decade. Not just in tech. Everywhere.
This post is a map of what I'm seeing. It's in bullet points because I want to show the connections, not argue a thesis. The thesis is the connections themselves.
The Coding Debates Are the Early Warning System
Jonas Rodrigues has written three posts that form a tight sequence: "Mastermind in the Machine", "Post-AI Development", and "The Vibe Schism." Together they track a real-time debate happening right now among software engineers.
"Vibe coding" means using AI to generate entire programs based on what you want the output to feel like, without writing or necessarily understanding the underlying code.
Jonas finds that developers using AI tools estimated they were 20% faster. When measured, they were actually 19% slower. That's a nearly 40-point gap between perception and reality.
He also finds a 17% decrease in developer mastery--people scoring lower on understanding the very code they just told AI to generate.
And he finds that AI-generated code introduces security vulnerabilities 45% of the time. The code compiles. The code runs. But it has hidden holes that can be exploited.
Here's the key: coding is the most deterministic domain where AI is being used. Code either compiles or it doesn't. Tests pass or fail. Security vulnerabilities can be counted. These problems are visible in coding first because coding is the field where failure is easiest to measure.
That doesn't mean these problems are limited to coding. It means coding is the canary in the coal mine.
The Same Pattern in Sports
Zay Amaro's "The Simulation Trap" asks: if a coach has seen the optimal play ten million times in simulation, do they still have freedom to follow their gut? Or does the data create a trap where deviating from the algorithm is professionally riskier than following it, even when your instincts say otherwise?
Kevion Milton's "The Cold Doctor" describes an athlete whose AI dashboard cleared his knee at 100% structural integrity after an ACL tear. The data said he was stronger than before. But every time he cut left, he hesitated. He was terrified. The AI couldn't measure fear. Because the data said "healthy," the coaches pushed him. He compensated, ran wrong, and tore his other hamstring.
Kevion's "Health vs. Hype" tracks the same split Jonas found in coding: AI is genuinely great at injury prevention (14% drop in lower-extremity strains league-wide). The upside is real. But the system also creates new failure modes--biometric data that could tank contract negotiations, "cognitive load monitoring" that quantifies focus in ways players can't control or contest.
Sam Levine's work on AI in rugby training and risk automation in sports raises the question of what happens when the tools meant to keep athletes safe also change who has authority over their bodies.
Kevion's "The Myth of Objective Perfection" extends this to officiating: if AI calls every foul with mathematical certainty, you eliminate human error--but you also eliminate the narrative tension that makes sports worth watching. You can't argue with a sensor. And arguing about the calls is half the experience.
Notice the pattern: the surface layer gets better (fewer injuries, faster scouting, more data), but a deeper layer gets harder to see and harder to talk about (fear, autonomy, who owns the data, what makes the game meaningful).
The Bridge: Isabella's Cross-Domain Work
Isabella Calmet is the writer in the network who is most explicitly drawing lines between domains. Her "Beyond the Vibe" post takes Jonas's coding vocabulary--"insecure spaghetti," "mastery atrophy," "Calibrated Trust"--and maps it directly onto medicine and athletics.
She proposes a "50/50 Rule": the machine handles 50% (data processing, pattern detection, boilerplate), the human owns 50% (judgment, ethics, final evaluation). This is her attempt to answer the skill atrophy question in domain-general terms.
In "The Infallible Diagnostician," she notes that AI achieves 94% diagnostic accuracy in some medical contexts. That's real. That saves lives. But without a human understanding the genomic "interfaces," you risk "off-target effects"--the medical equivalent of Jonas's security vulnerabilities.
Her "Informed Instinct" post argues that what we call "pure instinct" in athletics is often just "uninformed risk." AI can turn that into informed risk. But who owns the information? She connects this to Indigenous data sovereignty--the principle that data about you should be controlled by you, not by the platform that collected it.
What Isabella is doing: showing that the same structural problem appears whether you're talking about a developer who can't debug their own AI-generated code, a doctor who can't explain why the AI flagged a gene sequence, or an athlete whose biometric data is used against them in contract negotiations. The surface gets better. The depth gets opaque. And the question of who maintains understanding--and therefore agency--becomes urgent.
The Music Warning
Jonas's "Gamification of Music" post extends the pattern into yet another domain. AI music platforms turn songwriting into a text-prompt slot machine. The CEO of Suno has explicitly said the goal is to make the music industry as big as the gaming industry.
Jonas identifies "cognitive atrophy"--when you never struggle with an instrument, the musical intuitions developed over centuries of practice begin to wither.
He identifies the "narcissistic feedback loop"--users primarily listening to tracks they just generated, fracturing shared culture into millions of sealed echo chambers.
And he identifies the audience retraining problem: if a generation grows up on instant AI-generated music, a ninety-minute set by a human band will feel unbearably slow to a brain rewired for immediate engagement.
This is the same pattern again: the surface layer is cheap and abundant (anyone can "make" a song). The deep layer atrophies (the skill, the shared experience, the patience for complexity). And new failure modes emerge that are hard to see until the damage is done.
So What About Writing?
Here's where it gets personal for this class.
Everything Jonas documents about coding applies to writing. "Vibe writing" is already a thing--prompting AI to generate essays based on what you want the output to feel like, without constructing the argument yourself.
The productivity paradox almost certainly applies. It feels faster. Whether it actually is--when you account for the back-and-forth of prompting, evaluating, re-prompting, and the time you don't spend learning--is an open question.
The mastery atrophy almost certainly applies. If you never struggle with how to structure a paragraph, how to transition between ideas, how to make a source do work inside your argument, those capacities don't develop. Just as the developer scores 17% lower on understanding their own AI-generated code.
But here's the difference, and it's the reason I think writing is behind coding in reckoning with this: writing has no compiler.
In coding, security vulnerabilities are dangerous because they look like working code. The program compiles. It runs. But there's a hidden flaw that can be exploited. The surface passes. The depth is compromised. And eventually, it crashes in production.
In writing, the equivalent would be prose that reads fluently, cites sources, follows paragraph structure, and produces the surface appearance of reasoning--without any load-bearing argument underneath. It "compiles." It passes the vibe check. But there's no one home.
The difference: bad code eventually crashes. Bad arguments just... circulate. There is no production server that throws an error when an essay's reasoning is hollow. There is no test suite for whether a writer actually understands their own sources.
This makes the "security problem" in writing arguably worse than in coding, because the failure mode is invisible. The equivalent of a security vulnerability in writing is an argument that looks convincing but can't withstand scrutiny--and in most contexts, no one applies scrutiny. It just passes.
If 45% of AI-generated code has security vulnerabilities, what percentage of AI-generated argumentative prose has "argumentative vulnerabilities"--claims that sound right but don't hold up, sources that are gestured at but not actually engaged, logical structures that look like reasoning but aren't? We don't have a number because we don't have a scanner. But the structural problem is identical.
What This Means (and What It Doesn't)
This is not an AI vs. human argument. That debate is over. It has been over. AI is here, the upsides are real, and the question of whether to use it is not a serious question anymore. The NFL's 14% injury reduction is real. The diagnostic accuracy is real. The speed of generating a working prototype is real.
The question is: what happens next? Every domain is going to face the same set of problems, on a timeline proportional to how easy it is to measure quality in that domain. Coding is first because failure is most visible there. Sports is close behind (you can count injuries, wins, losses). Writing is further back. Fields like therapy, management, education, and law may be the last to reckon with it--because quality in those fields is hardest to measure, which means the damage is hardest to see.
The upsides are real and they're already here. Nobody should pretend otherwise. The conversation isn't about going back.
The downsides are real and they're emerging fast. Skill atrophy. The productivity illusion. Security vulnerabilities that hide inside fluent surfaces. The concentration of understanding in fewer and fewer people. Data ownership questions. The retraining of audiences and users to prefer the fast and shallow over the slow and deep.
There is no way out except through. We don't get to skip these problems. We have to name them, study them, and start developing frameworks for dealing with them. Isabella's 50/50 Rule is one attempt. Jonas's "Calibrated Trust" is another. Zay's insistence on preserving space for human judgment is a third. None of them are finished answers. All of them are the right kind of thinking.
The best tool we have right now is analogy. When you notice that the "insecure spaghetti" problem in coding looks like the "off-target effects" problem in gene editing looks like the "kinesiophobia" problem in sports medicine looks like the "argumentative vulnerability" problem in writing--you're doing something important. You're seeing the shared structure underneath domain-specific debates. That's how you get ahead of a problem instead of being surprised by it every time it shows up in a new field.
Questions Worth Thinking About
What are the skills in your domain that people will say "you don't need to learn that anymore"? Which of those claims will turn out to be right, and which will turn out to be a version of "you don't need to learn to navigate because GPS exists"?
What does "vibe writing" look like? What does "vibe doctoring" look like? "Vibe teaching"? "Vibe lawyering"? At what point does "vibing" become a real new skill, and at what point is it just not doing the work?
What is the "security vulnerability" in your field--the failure mode that hides inside a surface that looks fine? In coding it's exploitable flaws in running programs. In writing it might be hollow arguments in fluent prose. In medicine it might be correct diagnoses with missed context. In sports it might be optimized performance with unmeasured psychological damage. What's the version of this in whatever you end up doing professionally?
Jonas's most striking finding: the most valuable engineers of the next decade won't be the fastest at prompting AI. They'll be the "10x Verifiers"--people with enough deep understanding to look at AI-generated output and spot the one flaw that matters. Is there a version of this in every field? What would a "10x Verifier" look like in writing, in medicine, in law?
Who owns the understanding? When AI handles the surface, and fewer and fewer people understand the depth, who has power? Isabella's data sovereignty question isn't just about rugby players' biometric data. It's about whether the people using AI tools retain enough understanding to evaluate what those tools produce--or whether they become, as Jonas puts it, "passengers in a vehicle they cannot steer."
This is the conversation. Not AI versus human. Not ban it versus embrace it. The conversation is: this is happening, it's happening fast, the upsides are genuine, the downsides are emerging, and the only way to deal with it is to look at it clearly--across domains, through analogies, with the willingness to sit in the discomfort of not having clean answers yet.
You're already doing it. That's what the blog network is for.
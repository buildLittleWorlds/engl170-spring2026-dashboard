The GPS and the Driver: A Response to Emani Gerdine
January 16, 2026
In my previous post, The AI Balancing Act, I argued that we must treat Artificial Intelligence as a tool rather than a crutch to prevent our own cognitive skills from atrophying. I warned against the "beige" content that comes from outsourcing our thinking.
However, after reading my classmate Emani Gerdine’s insightful post, AI as a Cognitive Partner, Not a Shortcut, I realized that the conversation requires even more nuance. Emani moves the debate past the binary of "cheating vs. struggle" and introduces the concept of a "third space."
This idea of the "Cognitive Partner" is fascinating, and I want to explore how her concept of the "GPS" aligns—and potentially conflicts—with my fear of skill atrophy.
The GPS Analogy: Navigation vs. Knowing the Road
Emani writes: "In this context, AI functions like a 'GPS' for a researcher—it doesn’t take the journey for you, but it helps ensure you don’t get lost in the forest of your own notes."
This is a perfect extension of the "Co-Pilot" metaphor I used previously. The GPS doesn't drive the car; it simply optimizes the route. It removes the friction of getting lost so the driver can focus on the road conditions and safety.
However, there is a risk hidden in this metaphor. We’ve all heard stories of people who blindly followed their GPS into a lake or down a closed road because they stopped looking at the actual terrain. If we rely on the AI "GPS" to organize every thought (the "architecture of the mind" that Emani references via Dr. Rob Lively), do we lose our internal sense of direction?
I agree with Emani that using AI to structure an outline is not "outsourcing," but I would add a caveat: We must be able to recognize when the GPS is wrong. The "Cognitive Partner" is only valuable if the human partner has enough expertise to say, "No, that route doesn't make sense."
Ethical Opacity and the Final Product
Perhaps the most provocative point Emani raises is the concept of "Ethical Opacity." She argues: "It is undeniable that some individuals use AI to generate entire papers... However, treating every instance of AI use as 'cheating' ignores the reality of 'ethical opacity'—the idea that a creator should be judged on their final product and the accuracy of their claims, rather than being surveilled during the creative act."
This challenges my previous stance slightly. In "The AI Balancing Act," I focused heavily on the process—the "manual mode" days and the "struggle" of writing. Emani argues that if the output is accurate, rigorous, and professional (like a physical therapy treatment plan), the process matters less than the result.
I think the middle ground here is ownership. Emani mentions that "The AI provided the ladder, but the student did the climbing." This is the key. "Ethical Opacity" is acceptable only if the student actually climbed the ladder. If the student simply teleported to the top (copy-pasting the output), they haven't just cheated the assignment; they have cheated their future patients who rely on that knowledge being embedded in the student's brain, not just in their ChatGPT history.
The Professional Standard
Emani’s example of the healthcare field is striking. She notes that in a clinical setting, she will be "expected to use every resource available to help a patient recover."
This aligns perfectly with my argument that the future belongs to those who know how to use the tool. A doctor who refuses to use diagnostic AI because they want to "do it the old fashioned way" is likely providing subpar care. But a doctor who relies only on the AI without understanding the physiology behind it is dangerous.
Conclusion: Merging the Pilot and the Partner
Emani and I are essentially circling the same truth from different altitudes. I am worried about the pilot forgetting how to fly; she is focused on the pilot having the best possible navigation system.
To combine our views: We should embrace AI as the "Cognitive Partner" and the "GPS," but we must never turn off our own internal compass. We use the checklist to ensure rigor, but we use our human judgment to ensure meaning.
As we move through this semester, I look forward to testing this "third space." Can we use these tools to build the architecture of our minds without letting the robots become the architects?